{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import numpy as np\n",
    "import chess\n",
    "import chess.pgn as pgn\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from model import ChessModel\n",
    "from processing_parallel import collect_unique_moves_parallel, preprocess_and_save_to_hdf5_parallel\n",
    "from dataset import HDF5Dataset\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to PGN files\n",
    "data_dir = '../../games/data/pgn'\n",
    "files = [os.path.join(data_dir, file) for file in os.listdir(data_dir) if file.endswith('.pgn')]\n",
    "files.sort()  # Ensure consistent order\n",
    "\n",
    "# Parameters\n",
    "LIMIT_OF_FILES = min(len(files), 28)\n",
    "files = files[:LIMIT_OF_FILES]\n",
    "max_games = 500000\n",
    "positions_per_game = 10\n",
    "batch_size = 128\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_to_int, num_classes = collect_unique_moves_parallel(files, max_games=max_games)\n",
    "\n",
    "# Save move_to_int mapping for future use\n",
    "with open('mark5_move_to_int.pkl', 'wb') as f:\n",
    "    pickle.dump(move_to_int, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the move_to_int mapping\n",
    "with open(\"../mark5_move_to_int.pkl\", \"rb\") as file:\n",
    "    move_to_int = pickle.load(file)\n",
    "num_classes = len(move_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "130 minutes - 638 k lines of h5; 140 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_save_to_hdf5_parallel(files, move_to_int, max_games, positions_per_game=positions_per_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "dataset = HDF5Dataset('../preprocessed_data.h5')\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, num_workers=0\n",
    ")\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "model = ChessModel(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "max_lr = 0.01  # You can adjust this based on experimentation\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=max_lr,\n",
    "    steps_per_epoch=len(dataset) // batch_size + 1,\n",
    "    epochs=num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't read data (addr overflow, addr = 1672, size = 3656, eoa = 2048)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/maksiuuuuuuu/Desktop/MyWork/game_bots/chess_bot_fr/base_cnn_model/train/train_hdf5.ipynb Cell 8\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maksiuuuuuuu/Desktop/MyWork/game_bots/chess_bot_fr/base_cnn_model/train/train_hdf5.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maksiuuuuuuu/Desktop/MyWork/game_bots/chess_bot_fr/base_cnn_model/train/train_hdf5.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pbar \u001b[39m=\u001b[39m tqdm(data_loader, desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/maksiuuuuuuu/Desktop/MyWork/game_bots/chess_bot_fr/base_cnn_model/train/train_hdf5.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;49;00m X_batch, y_batch \u001b[39min\u001b[39;49;00m pbar:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maksiuuuuuuu/Desktop/MyWork/game_bots/chess_bot_fr/base_cnn_model/train/train_hdf5.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     X_batch \u001b[39m=\u001b[39;49m X_batch\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maksiuuuuuuu/Desktop/MyWork/game_bots/chess_bot_fr/base_cnn_model/train/train_hdf5.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     y_batch \u001b[39m=\u001b[39;49m y_batch\u001b[39m.\u001b[39;49mto(device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;49;00m obj \u001b[39min\u001b[39;49;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;49;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Desktop/MyWork/game_bots/chess_bot_fr/base_cnn_model/train/dataset.py:20\u001b[0m, in \u001b[0;36mHDF5Dataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[1;32m     19\u001b[0m     \u001b[39mwith\u001b[39;00m h5py\u001b[39m.\u001b[39mFile(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhdf5_file_path, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m h5_file:\n\u001b[0;32m---> 20\u001b[0m         X \u001b[39m=\u001b[39m h5_file[\u001b[39m'\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m'\u001b[39;49m][idx]\n\u001b[1;32m     21\u001b[0m         y \u001b[39m=\u001b[39m h5_file[\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m][idx]\n\u001b[1;32m     23\u001b[0m     \u001b[39m# Data Augmentation: Randomly flip the board\u001b[39;00m\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/h5py/_hl/dataset.py:758\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fast_read_ok \u001b[39mand\u001b[39;00m (new_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    757\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 758\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fast_reader\u001b[39m.\u001b[39;49mread(args)\n\u001b[1;32m    759\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    760\u001b[0m         \u001b[39mpass\u001b[39;00m  \u001b[39m# Fall back to Python read pathway below\u001b[39;00m\n",
      "File \u001b[0;32mh5py/_selector.pyx:376\u001b[0m, in \u001b[0;36mh5py._selector.Reader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't read data (addr overflow, addr = 1672, size = 3656, eoa = 2048)"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(data_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
    "    for X_batch, y_batch in pbar:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step() \n",
    "\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "        pbar.set_postfix({'Loss': running_loss / ((pbar.n + 1) * batch_size)})\n",
    "\n",
    "    epoch_loss = running_loss / len(dataset)\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    minutes = int(epoch_time // 60)\n",
    "    seconds = int(epoch_time % 60)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Time: {minutes}m{seconds}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"mark5-10e-500k.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['X', 'y']\n",
      "X Shape: (5000000, 16, 8, 8)\n",
      "y Shape: (5000000,)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "hdf5_file_path = \"../preprocessed_data.h5\"\n",
    "with h5py.File(hdf5_file_path, 'r') as h5_file:\n",
    "    print(\"Keys:\", list(h5_file.keys()))\n",
    "    print(\"X Shape:\", h5_file['X'].shape)\n",
    "    print(\"y Shape:\", h5_file['y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error accessing HDF5 file: Can't read data (addr overflow, addr = 1672, size = 3656, eoa = 2048)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "try:\n",
    "    with h5py.File(hdf5_file_path, 'r') as h5_file:\n",
    "        h5_file['X'][:10]  # Attempt to read a small portion\n",
    "        h5_file['y'][:10]\n",
    "except Exception as e:\n",
    "    print(\"Error accessing HDF5 file:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
