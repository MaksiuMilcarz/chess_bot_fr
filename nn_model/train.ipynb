{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo\n",
    "* increase chunk size - 5k+\n",
    "* increase games count\n",
    "* adaptive learning rate\n",
    "* batch normalization\n",
    "* focus on mid-end game\n",
    "* more comment tips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # type: ignore\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn # type: ignore\n",
    "import torch.optim as optim # type: ignore\n",
    "from torch.optim.lr_scheduler import OneCycleLR # type: ignore\n",
    "import math\n",
    "from torch.utils.data import DataLoader # type: ignore\n",
    "from chess import pgn # type: ignore\n",
    "import tqdm # type: ignore\n",
    "from dataset import ChessDataset\n",
    "from model import ChessModel\n",
    "from helper_funcs import collect_unique_moves, create_input_for_nn\n",
    "from helper_funcs import process_data_and_save_chunks\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data - into chunks so that memory is not overwhelmed, store them in sepearte folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [os.path.join(\"../data/pgn\", file) for file in os.listdir(\"../data/pgn\") if file.endswith(\".pgn\")]\n",
    "files.sort()  # Ensure consistent order\n",
    "LIMIT_OF_FILES = min(len(files), 28)\n",
    "files = files[:LIMIT_OF_FILES]\n",
    "\n",
    "max_games = 150000\n",
    "chunk_size = 15000\n",
    "\n",
    "# Collect unique moves\n",
    "move_to_int, num_classes = collect_unique_moves(files, max_games=max_games)\n",
    "\n",
    "with open(\"../../models/mark3_move_to_int\", \"wb\") as file:\n",
    "    pickle.dump(move_to_int, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chunk_files = process_data_and_save_chunks(files, move_to_int, chunk_size=chunk_size, max_games=max_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using MPS backend on Apple Silicon (M2).\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"MPS backend not available. Using CPU.\")\n",
    "    \n",
    "# Model Initialization\n",
    "model = ChessModel(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "batch_size = 64  # Ensure this matches your DataLoader batch_size\n",
    "total_batches_per_epoch = 0\n",
    "for data_chunk_file in data_chunk_files:\n",
    "    data = np.load(data_chunk_file)\n",
    "    num_samples = data['X'].shape[0]\n",
    "    num_batches = math.ceil(num_samples / batch_size)\n",
    "    total_batches_per_epoch += num_batches\n",
    "\n",
    "total_steps = num_epochs * total_batches_per_epoch\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.001, total_steps=total_steps)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_batches = 0\n",
    "    for data_chunk_file in tqdm.tqdm(data_chunk_files, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        # Load data chunk\n",
    "        data = np.load(data_chunk_file)\n",
    "        X = torch.tensor(data['X'], dtype=torch.float32)\n",
    "        y = torch.tensor(data['y'], dtype=torch.long)\n",
    "        # Create Dataset and DataLoader\n",
    "        dataset = ChessDataset(X, y)\n",
    "        dataloader = DataLoader(dataset, batch_size=64, num_workers=4, shuffle=True)\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to device\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)  # Raw logits\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate\n",
    "            running_loss += loss.item()\n",
    "            total_batches += 1\n",
    "        # Free up memory\n",
    "        del X, y, dataset, dataloader\n",
    "        gc.collect()\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    minutes = int(epoch_time // 60)\n",
    "    seconds = int(epoch_time % 60)\n",
    "    avg_loss = running_loss / total_batches\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, Time: {minutes}m{seconds}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"../models/mark3-50e-150k.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mark 1 - 10e - 50k\n",
    "* base; 5k chunks, 14 boards reprezentation\n",
    "* 10 minutes per epoch - 50k games\n",
    "\n",
    "# Mark 2 - 20e - 100k\n",
    "* 10k chunks\n",
    "* adaptive learning rate\n",
    "* probabilistic favourizm of mid-late game states\n",
    "* batch normalization after convolutional layer\n",
    "\n",
    "# Mark 3\n",
    "* 150k - 15k chunks - 50e\n",
    "* its useable :D - forgot to load the mapping and data is sampled randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
