{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo\n",
    "* adaptive learning rate\n",
    "* batch normalization\n",
    "* focus on mid-end game\n",
    "* save tensors to chunks and train on bigger datasets\n",
    "* more comment tips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # type: ignore\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn # type: ignore\n",
    "import torch.optim as optim # type: ignore\n",
    "from torch.utils.data import DataLoader # type: ignore\n",
    "from chess import pgn # type: ignore\n",
    "from tqdm import tqdm # type: ignore\n",
    "from dataset import ChessDataset\n",
    "from model import ChessModel\n",
    "from helper_funcs import create_input_for_nn, encode_moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data - into chunks so that memory is not overwhelmed, store them in sepearte folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [13:47<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAMES PARSED: 315135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_pgn(file_path):\n",
    "    games = []\n",
    "    with open(file_path, 'r') as pgn_file:\n",
    "        while True:\n",
    "            game = pgn.read_game(pgn_file)\n",
    "            if game is None:\n",
    "                break\n",
    "            games.append(game)\n",
    "    return games\n",
    "\n",
    "files = [file for file in os.listdir(\"../data/pgn\") if file.endswith(\".pgn\")]\n",
    "LIMIT_OF_FILES = min(len(files), 28)\n",
    "games = []\n",
    "i = 1\n",
    "for file in tqdm(files):\n",
    "    games.extend(load_pgn(f\"../data/pgn/{file}\"))\n",
    "    if i >= LIMIT_OF_FILES:\n",
    "        break\n",
    "    i += 1\n",
    "    \n",
    "print(f\"GAMES PARSED: {len(games)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "X, y = create_input_for_nn(games)\n",
    "\n",
    "print(f\"NUMBER OF SAMPLES: {len(y)}\")\n",
    "\n",
    "X = X[0:2500000]\n",
    "y = y[0:2500000]\n",
    "\n",
    "y, move_to_int = encode_moves(y)\n",
    "num_classes = len(move_to_int)\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS backend on Apple Silicon (M2).\n"
     ]
    },
    {
     "ename": "TimeoutError",
     "evalue": "[Errno 60] Operation timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/maksiuuuuuuu/Desktop/MyWork/game_bots/chess_bot_fr/nn_model/train.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maksiuuuuuuu/Desktop/MyWork/game_bots/chess_bot_fr/nn_model/train.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMPS backend not available. Using CPU.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maksiuuuuuuu/Desktop/MyWork/game_bots/chess_bot_fr/nn_model/train.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Create dataset\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/maksiuuuuuuu/Desktop/MyWork/game_bots/chess_bot_fr/nn_model/train.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m dataset \u001b[39m=\u001b[39m ChunkedChessDataset(data_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdata_chunks\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maksiuuuuuuu/Desktop/MyWork/game_bots/chess_bot_fr/nn_model/train.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m dataset\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maksiuuuuuuu/Desktop/MyWork/game_bots/chess_bot_fr/nn_model/train.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m train_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_dataset)\n",
      "File \u001b[0;32m~/Desktop/MyWork/game_bots/chess_bot_fr/nn_model/dataset.py:20\u001b[0m, in \u001b[0;36mChunkedChessDataset.__init__\u001b[0;34m(self, data_dir)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39m# Calculate cumulative sizes for indexing\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m chunk_file \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_files:\n\u001b[0;32m---> 20\u001b[0m     data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(chunk_file)\n\u001b[1;32m     21\u001b[0m     chunk_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data[\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     22\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m chunk_size\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:1066\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1063\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1065\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m-> 1066\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1067\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n\u001b[1;32m   1071\u001b[0m         overall_storage \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:206\u001b[0m, in \u001b[0;36m_is_zipfile\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39m# Read the first few bytes and match against the ZIP file signature\u001b[39;00m\n\u001b[1;32m    205\u001b[0m local_header_magic_number \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPK\u001b[39m\u001b[39m\\x03\u001b[39;00m\u001b[39m\\x04\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 206\u001b[0m read_bytes \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m(local_header_magic_number))\n\u001b[1;32m    207\u001b[0m f\u001b[39m.\u001b[39mseek(start)\n\u001b[1;32m    208\u001b[0m \u001b[39mreturn\u001b[39;00m read_bytes \u001b[39m==\u001b[39m local_header_magic_number\n",
      "\u001b[0;31mTimeoutError\u001b[0m: [Errno 60] Operation timed out"
     ]
    }
   ],
   "source": [
    "# Create Dataset and DataLoader\n",
    "dataset = ChessDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Check for GPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using MPS backend on Apple Silicon (M2).\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"MPS backend not available. Using CPU.\")\n",
    "\n",
    "# Model Initialization\n",
    "model = ChessModel(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS backend on Apple Silicon (M2).\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in tqdm(dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)  # Raw logits\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    minutes: int = int(epoch_time // 60)\n",
    "    seconds: int = int(epoch_time) - minutes * 60\n",
    "    print(f'Epoch {epoch + 1 + 50}/{num_epochs + 1 + 50}, Loss: {running_loss / len(dataloader):.4f}, Time: {minutes}m{seconds}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Training:   0%|          | 17/145578 [03:52<553:00:22, 13.68s/batch]"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"../models/mark1-10e.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
