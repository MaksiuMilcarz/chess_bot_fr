{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo\n",
    "More quiality data:\n",
    "* take games with different openings to ensure variaty - maybe allow moer openings?\n",
    "* train only on roughly fair, equal possitions\n",
    "\n",
    "* sample best move from legal predicted ones not just take the best!!!\n",
    "\n",
    "* make mark3.3 play mark3\n",
    "\n",
    "* Java App for visulization\n",
    "\n",
    "* Re-enforcement Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # type: ignore\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn # type: ignore\n",
    "import torch.optim as optim # type: ignore\n",
    "from torch.optim.lr_scheduler import OneCycleLR # type: ignore\n",
    "import math\n",
    "from torch.utils.data import DataLoader # type: ignore\n",
    "from chess import pgn # type: ignore\n",
    "import tqdm # type: ignore\n",
    "from dataset import ChessDataset\n",
    "from model import ChessModel\n",
    "from helper_funcs import collect_unique_moves, create_input_for_nn\n",
    "from helper_funcs import process_data_and_save_chunks\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data - into chunks so that memory is not overwhelmed, store them in sepearte folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting Moves:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "files = [os.path.join(\"../data/pgn\", file) for file in os.listdir(\"../data/pgn\") if file.endswith(\".pgn\")]\n",
    "files.sort()  # Ensure consistent order\n",
    "LIMIT_OF_FILES = min(len(files), 28)\n",
    "files = files[:LIMIT_OF_FILES]\n",
    "\n",
    "max_games = 300000\n",
    "chunk_size = 10000\n",
    "\n",
    "# Collect unique moves\n",
    "move_to_int, num_classes = collect_unique_moves(files, max_games=max_games)\n",
    "\n",
    "with open(\"../models/mark3_move_to_int.pkl\", \"wb\") as file:\n",
    "    pickle.dump(move_to_int, file)\n",
    "    \n",
    "print(\"Number of classes: \", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data Chunks:   0%|          | 0/2 [16:40<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "data_chunk_files = process_data_and_save_chunks(files, move_to_int, chunk_size=chunk_size, max_games=max_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS backend on Apple Silicon (M2).\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using MPS backend on Apple Silicon (M2).\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"MPS backend not available. Using CPU.\")\n",
    "    \n",
    "# Model Initialization\n",
    "model = ChessModel(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|██████████| 30/30 [23:53<00:00, 47.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 3.1254, Time: 23m53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|██████████| 30/30 [24:04<00:00, 48.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25, Loss: 2.4327, Time: 24m4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|██████████| 30/30 [24:13<00:00, 48.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25, Loss: 2.3298, Time: 24m13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|██████████| 30/30 [24:36<00:00, 49.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25, Loss: 2.2630, Time: 24m36s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|██████████| 30/30 [24:34<00:00, 49.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25, Loss: 2.2040, Time: 24m34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|██████████| 30/30 [24:38<00:00, 49.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25, Loss: 2.1553, Time: 24m38s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|██████████| 30/30 [24:39<00:00, 49.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25, Loss: 2.1145, Time: 24m39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|██████████| 30/30 [24:59<00:00, 49.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25, Loss: 2.0776, Time: 24m59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|██████████| 30/30 [25:19<00:00, 50.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25, Loss: 2.0458, Time: 25m19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|██████████| 30/30 [25:21<00:00, 50.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25, Loss: 2.0198, Time: 25m21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|██████████| 30/30 [25:08<00:00, 50.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25, Loss: 1.9982, Time: 25m8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|██████████| 30/30 [25:23<00:00, 50.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25, Loss: 1.9785, Time: 25m23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|██████████| 30/30 [25:20<00:00, 50.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25, Loss: 1.9602, Time: 25m20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|██████████| 30/30 [25:32<00:00, 51.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25, Loss: 1.9427, Time: 25m32s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|██████████| 30/30 [25:27<00:00, 50.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25, Loss: 1.9259, Time: 25m27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25:  10%|█         | 3/30 [02:32<22:55, 50.94s/it]\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "No data left in file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/maksiuuuuuuu/Desktop/MyWork/game_bots/chess_bot_fr/nn_model/train.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maksiuuuuuuu/Desktop/MyWork/game_bots/chess_bot_fr/nn_model/train.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m total_batches \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maksiuuuuuuu/Desktop/MyWork/game_bots/chess_bot_fr/nn_model/train.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m data_chunk_file \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(data_chunk_files, desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maksiuuuuuuu/Desktop/MyWork/game_bots/chess_bot_fr/nn_model/train.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m# Load data chunk\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/maksiuuuuuuu/Desktop/MyWork/game_bots/chess_bot_fr/nn_model/train.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(data_chunk_file)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maksiuuuuuuu/Desktop/MyWork/game_bots/chess_bot_fr/nn_model/train.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     X \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(data[\u001b[39m'\u001b[39m\u001b[39mX\u001b[39m\u001b[39m'\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maksiuuuuuuu/Desktop/MyWork/game_bots/chess_bot_fr/nn_model/train.ipynb#X12sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(data[\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/numpy/lib/npyio.py:436\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    434\u001b[0m magic \u001b[39m=\u001b[39m fid\u001b[39m.\u001b[39mread(N)\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m magic:\n\u001b[0;32m--> 436\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo data left in file\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    437\u001b[0m \u001b[39m# If the file size is less than N, we need to make sure not\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[39m# to seek past the beginning of the file\u001b[39;00m\n\u001b[1;32m    439\u001b[0m fid\u001b[39m.\u001b[39mseek(\u001b[39m-\u001b[39m\u001b[39mmin\u001b[39m(N, \u001b[39mlen\u001b[39m(magic)), \u001b[39m1\u001b[39m)  \u001b[39m# back-up\u001b[39;00m\n",
      "\u001b[0;31mEOFError\u001b[0m: No data left in file"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "\n",
    "batch_size = 64  # Ensure this matches your DataLoader batch_size\n",
    "total_batches_per_epoch = 0\n",
    "for data_chunk_file in data_chunk_files:\n",
    "    data = np.load(data_chunk_file)\n",
    "    num_samples = data['X'].shape[0]\n",
    "    num_batches = math.ceil(num_samples / batch_size)\n",
    "    total_batches_per_epoch += num_batches\n",
    "\n",
    "total_steps = num_epochs * total_batches_per_epoch\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.001, total_steps=total_steps)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_batches = 0\n",
    "    for data_chunk_file in tqdm.tqdm(data_chunk_files, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        # Load data chunk\n",
    "        data = np.load(data_chunk_file)\n",
    "        X = torch.tensor(data['X'], dtype=torch.float32)\n",
    "        y = torch.tensor(data['y'], dtype=torch.long)\n",
    "        # Create Dataset and DataLoader\n",
    "        dataset = ChessDataset(X, y)\n",
    "        dataloader = DataLoader(dataset, batch_size=64, num_workers=4, shuffle=True)\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to device\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)  # Raw logits\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate\n",
    "            running_loss += loss.item()\n",
    "            total_batches += 1\n",
    "        # Free up memory\n",
    "        del X, y, dataset, dataloader\n",
    "        gc.collect()\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    minutes = int(epoch_time // 60)\n",
    "    seconds = int(epoch_time % 60)\n",
    "    avg_loss = running_loss / total_batches\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, Time: {minutes}m{seconds}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"../models/mark3-15e-300k.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mark 1 - 10e - 50k\n",
    "* base; 5k chunks, 14 boards reprezentation\n",
    "* 10 minutes per epoch - 50k games\n",
    "\n",
    "# Mark 2 - 20e - 100k\n",
    "* 10k chunks\n",
    "* adaptive learning rate\n",
    "* probabilistic favourizm of mid-late game states\n",
    "* batch normalization after convolutional layer\n",
    "\n",
    "# Mark 3\n",
    "* 300k - 10k chunks - 25e\n",
    "* its useable :D - forgot to load the mapping and data is sampled randomly\n",
    "* added additional garbage collection at the ends of the epochs\n",
    "* better to go for moer games "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue Training If crushed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS backend on Apple Silicon (M2).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/84/sbq4d0yx2vdcrx0_5_cy2hmh0000gn/T/ipykernel_10401/2942774120.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"../models/mark3-continued.pth\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import your custom modules\n",
    "from dataset import ChessDataset\n",
    "from model import ChessModel\n",
    "from helper_funcs import collect_unique_moves, process_data_and_save_chunks\n",
    "\n",
    "# Set device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using MPS backend on Apple Silicon (M2).\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"MPS backend not available. Using CPU.\")\n",
    "\n",
    "# Load the move_to_int mapping\n",
    "with open(\"../models/mark3_move_to_int.pkl\", \"rb\") as file:\n",
    "    move_to_int = pickle.load(file)\n",
    "num_classes = len(move_to_int)\n",
    "\n",
    "model = ChessModel(num_classes=num_classes).to(device)\n",
    "model.load_state_dict(torch.load(\"../models/mark3-continued.pth\", map_location=device))\n",
    "\n",
    "max_games = 300000\n",
    "chunk_size = 10000\n",
    "\n",
    "# Assuming data_chunk_files are available\n",
    "data_chunk_files = [f for f in os.listdir('.') if f.startswith('data_chunk_') and f.endswith('.npz')]\n",
    "data_chunk_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_chunk_20.npz loaded successfully\n",
      "data_chunk_21.npz loaded successfully\n",
      "data_chunk_23.npz loaded successfully\n",
      "data_chunk_22.npz loaded successfully\n",
      "data_chunk_26.npz loaded successfully\n",
      "data_chunk_27.npz loaded successfully\n",
      "data_chunk_19.npz loaded successfully\n",
      "data_chunk_25.npz loaded successfully\n",
      "data_chunk_24.npz loaded successfully\n",
      "data_chunk_18.npz loaded successfully\n",
      "data_chunk_8.npz loaded successfully\n",
      "data_chunk_9.npz loaded successfully\n",
      "data_chunk_2.npz loaded successfully\n",
      "data_chunk_3.npz loaded successfully\n",
      "data_chunk_1.npz loaded successfully\n",
      "data_chunk_0.npz loaded successfully\n",
      "data_chunk_4.npz loaded successfully\n",
      "data_chunk_5.npz loaded successfully\n",
      "data_chunk_7.npz loaded successfully\n",
      "data_chunk_6.npz loaded successfully\n",
      "data_chunk_15.npz loaded successfully\n",
      "data_chunk_29.npz loaded successfully\n",
      "data_chunk_28.npz loaded successfully\n",
      "data_chunk_14.npz loaded successfully\n",
      "data_chunk_16.npz loaded successfully\n",
      "data_chunk_17.npz loaded successfully\n",
      "data_chunk_13.npz loaded successfully\n",
      "data_chunk_12.npz loaded successfully\n",
      "data_chunk_10.npz loaded successfully\n",
      "data_chunk_11.npz loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data_chunk_files = [f for f in os.listdir('.') if f.startswith('data_chunk_') and f.endswith('.npz')]\n",
    "for file in data_chunk_files:\n",
    "    try:\n",
    "        with np.load(file) as data:\n",
    "            print(f\"{file} loaded successfully\")\n",
    "    except EOFError:\n",
    "        print(f\"{file} is corrupted or incomplete.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 30/30 [36:23<00:00, 72.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 1.8970, Time: 36m23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 30/30 [37:12<00:00, 74.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 1.9352, Time: 37m12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 30/30 [35:49<00:00, 71.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 1.8558, Time: 35m49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "additional_epochs = 3  \n",
    "batch_size = 64  \n",
    "total_batches_per_epoch = 0\n",
    "for data_chunk_file in data_chunk_files:\n",
    "    data = np.load(data_chunk_file)\n",
    "    num_samples = data['X'].shape[0]\n",
    "    num_batches = math.ceil(num_samples / batch_size)\n",
    "    total_batches_per_epoch += num_batches\n",
    "    data.close()\n",
    "\n",
    "total_steps = additional_epochs * total_batches_per_epoch\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.001, total_steps=total_steps)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(additional_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_batches = 0\n",
    "    for data_chunk_file in tqdm(data_chunk_files, desc=f'Epoch {epoch+1}/{additional_epochs}'):\n",
    "        # Load data chunk\n",
    "        data = np.load(data_chunk_file)\n",
    "        X = torch.tensor(data['X'], dtype=torch.float32)\n",
    "        y = torch.tensor(data['y'], dtype=torch.long)\n",
    "        data.close()\n",
    "        # Create Dataset and DataLoader\n",
    "        dataset = ChessDataset(X, y)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=4, shuffle=True)\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to device\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)  # Raw logits\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate\n",
    "            running_loss += loss.item()\n",
    "            total_batches += 1\n",
    "        # Free up memory\n",
    "        del X, y, dataset, dataloader\n",
    "        gc.collect()\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    minutes = int(epoch_time // 60)\n",
    "    seconds = int(epoch_time % 60)\n",
    "    avg_loss = running_loss / total_batches\n",
    "    print(f'Epoch {epoch + 1}/{additional_epochs}, Loss: {avg_loss:.4f}, Time: {minutes}m{seconds}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated model\n",
    "torch.save(model.state_dict(), \"../models/mark3.3-19e.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
